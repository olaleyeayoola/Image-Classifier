{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shapes_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40e67afb5bb8467fae1d5533739320b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_476c2116ddf84a1a898c063427fedf49",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5da33743ddfe4a1c814b640a6984899c",
              "IPY_MODEL_40f380aec20f43b0a2dcceacc68017fe"
            ]
          }
        },
        "476c2116ddf84a1a898c063427fedf49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5da33743ddfe4a1c814b640a6984899c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1090d67304ae468094c737a7af074b38",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_981ce7107f3c48eca59bdd230abe8114"
          }
        },
        "40f380aec20f43b0a2dcceacc68017fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_48af63e3554945a5b0e4ea5cae933e37",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 97.8M/97.8M [00:00&lt;00:00, 153MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06f23c1162404e518879a3f504ec7ab9"
          }
        },
        "1090d67304ae468094c737a7af074b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "981ce7107f3c48eca59bdd230abe8114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "48af63e3554945a5b0e4ea5cae933e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06f23c1162404e518879a3f504ec7ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaleyeayoola/Image-Classifier/blob/master/shapes_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F2v6CHphdmS",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQnuE_HCbJ6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PyTorch Libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U9V-QAthi7C",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp3iIb0MbhqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Other Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import os  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74tp3JsJhk0J",
        "colab_type": "text"
      },
      "source": [
        "# Define Paths and Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oft-jhUAj2KH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip File\n",
        "from zipfile import ZipFile\n",
        "# Create a ZipFile Object and load sample.zip in it\n",
        "with ZipFile('shapes.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhbvxsIpb1eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'shapes'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1zgyeo_cM8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "40e67afb5bb8467fae1d5533739320b4",
            "476c2116ddf84a1a898c063427fedf49",
            "5da33743ddfe4a1c814b640a6984899c",
            "40f380aec20f43b0a2dcceacc68017fe",
            "1090d67304ae468094c737a7af074b38",
            "981ce7107f3c48eca59bdd230abe8114",
            "48af63e3554945a5b0e4ea5cae933e37",
            "06f23c1162404e518879a3f504ec7ab9"
          ]
        },
        "outputId": "e9d46afa-f279-4dbe-a8f5-126a3da22bc7"
      },
      "source": [
        "model = torchvision.models.resnet50(pretrained=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40e67afb5bb8467fae1d5533739320b4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=102502400), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1OEjC22cqJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(data_path):\n",
        "    \n",
        "    # Resize to 256 x 256, center-crop to 224x224 (to match the resnet image size), and convert to Tensor\n",
        "    transformation = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load all of the images, transforming them\n",
        "    full_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=data_path,\n",
        "        transform=transformation\n",
        "    )\n",
        "    \n",
        "    # Split into training (70%) and testing (30%) datasets)\n",
        "    train_size = int(0.7 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "    \n",
        "    # define a loader for the training data we can iterate through in 32-image batches\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=4,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    # define a loader for the testing data we can iterate through in 32-image batches\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=4,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "        \n",
        "    return train_loader, test_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiWNJ6NLc06Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define our class names\n",
        "classes = ['circle', 'square', 'triangle']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14_4__M7c4Ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the iterative dataloaders for test and training data\n",
        "train_loader, test_loader = load_dataset(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCr6HydEkrU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        data, target = data.to('cuda'), target.to('cuda')\n",
        "        \n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Push the data forward through the model layers\n",
        "        output = model(data)\n",
        "        \n",
        "        # Get the loss\n",
        "        loss = loss_criteria(output, target)\n",
        "\n",
        "        # Keep a running total\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print metrics so we see some progress\n",
        "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
        "            \n",
        "    # return average loss for the epoch\n",
        "    avg_loss = train_loss / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqxFijGpk0cV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(\"cuda\"), target.to(\"cuda\")\n",
        "            # Get the predicted classes for this batch\n",
        "            output = model(data)\n",
        "            # calculate the loss and successful predictions for this batch\n",
        "            test_loss += loss_criteria(output, target).item()\n",
        "            pred = output.max(1, keepdim=True)[1] \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    # return average loss for the epoch\n",
        "    return test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfiHx9G-lEMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an instance of the model class and allocate it to the device\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "# (see https://pytorch.org/docs/stable/optim.html#algorithms for details of supported algorithms)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Specify the loss criteria\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track metrics in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F89KHIc6lHMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd8f3177-b1c6-4a11-c082-b73b4826f926"
      },
      "source": [
        "# Train over 3 epochs\n",
        "epochs = 3\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(model, train_loader, optimizer, epoch)\n",
        "    test_loss = test(model, test_loader)\n",
        "    epoch_nums.append(epoch)\n",
        "    training_loss.append(train_loss)\n",
        "    validation_loss.append(test_loss)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "\tTraining batch 1 Loss: 8.485950\n",
            "\tTraining batch 2 Loss: 5.219261\n",
            "\tTraining batch 3 Loss: 0.934158\n",
            "\tTraining batch 4 Loss: 1.464446\n",
            "\tTraining batch 5 Loss: 0.016683\n",
            "\tTraining batch 6 Loss: 3.923137\n",
            "\tTraining batch 7 Loss: 1.940982\n",
            "\tTraining batch 8 Loss: 6.539708\n",
            "\tTraining batch 9 Loss: 1.317582\n",
            "\tTraining batch 10 Loss: 0.522532\n",
            "\tTraining batch 11 Loss: 0.200449\n",
            "\tTraining batch 12 Loss: 0.767241\n",
            "\tTraining batch 13 Loss: 0.117104\n",
            "\tTraining batch 14 Loss: 0.092376\n",
            "\tTraining batch 15 Loss: 0.505931\n",
            "\tTraining batch 16 Loss: 0.691702\n",
            "\tTraining batch 17 Loss: 0.127622\n",
            "\tTraining batch 18 Loss: 0.293167\n",
            "\tTraining batch 19 Loss: 0.139506\n",
            "\tTraining batch 20 Loss: 0.131469\n",
            "\tTraining batch 21 Loss: 0.259092\n",
            "\tTraining batch 22 Loss: 0.609505\n",
            "\tTraining batch 23 Loss: 0.005195\n",
            "\tTraining batch 24 Loss: 0.018751\n",
            "\tTraining batch 25 Loss: 0.166603\n",
            "\tTraining batch 26 Loss: 0.001794\n",
            "\tTraining batch 27 Loss: 0.757545\n",
            "\tTraining batch 28 Loss: 0.827014\n",
            "\tTraining batch 29 Loss: 0.333795\n",
            "\tTraining batch 30 Loss: 0.079843\n",
            "\tTraining batch 31 Loss: 0.023575\n",
            "\tTraining batch 32 Loss: 0.001176\n",
            "\tTraining batch 33 Loss: 0.369997\n",
            "\tTraining batch 34 Loss: 0.402224\n",
            "\tTraining batch 35 Loss: 0.648237\n",
            "\tTraining batch 36 Loss: 0.882554\n",
            "\tTraining batch 37 Loss: 0.004812\n",
            "\tTraining batch 38 Loss: 0.009233\n",
            "\tTraining batch 39 Loss: 0.027678\n",
            "\tTraining batch 40 Loss: 0.028558\n",
            "\tTraining batch 41 Loss: 0.173103\n",
            "\tTraining batch 42 Loss: 0.292917\n",
            "\tTraining batch 43 Loss: 0.494384\n",
            "\tTraining batch 44 Loss: 0.014506\n",
            "\tTraining batch 45 Loss: 0.073212\n",
            "\tTraining batch 46 Loss: 0.184829\n",
            "\tTraining batch 47 Loss: 2.181156\n",
            "\tTraining batch 48 Loss: 1.075117\n",
            "\tTraining batch 49 Loss: 0.047714\n",
            "\tTraining batch 50 Loss: 1.782125\n",
            "\tTraining batch 51 Loss: 0.196840\n",
            "\tTraining batch 52 Loss: 2.145584\n",
            "\tTraining batch 53 Loss: 0.030038\n",
            "\tTraining batch 54 Loss: 0.265448\n",
            "\tTraining batch 55 Loss: 0.033835\n",
            "\tTraining batch 56 Loss: 0.255650\n",
            "\tTraining batch 57 Loss: 0.011536\n",
            "\tTraining batch 58 Loss: 0.591409\n",
            "\tTraining batch 59 Loss: 0.565384\n",
            "\tTraining batch 60 Loss: 4.459396\n",
            "\tTraining batch 61 Loss: 0.136240\n",
            "\tTraining batch 62 Loss: 1.714381\n",
            "\tTraining batch 63 Loss: 1.033182\n",
            "\tTraining batch 64 Loss: 2.056212\n",
            "\tTraining batch 65 Loss: 0.110390\n",
            "\tTraining batch 66 Loss: 0.178437\n",
            "\tTraining batch 67 Loss: 0.207956\n",
            "\tTraining batch 68 Loss: 0.064483\n",
            "\tTraining batch 69 Loss: 0.154258\n",
            "\tTraining batch 70 Loss: 0.224392\n",
            "\tTraining batch 71 Loss: 0.464925\n",
            "\tTraining batch 72 Loss: 0.088118\n",
            "\tTraining batch 73 Loss: 1.004206\n",
            "\tTraining batch 74 Loss: 0.697514\n",
            "\tTraining batch 75 Loss: 0.428666\n",
            "\tTraining batch 76 Loss: 0.164877\n",
            "\tTraining batch 77 Loss: 0.059428\n",
            "\tTraining batch 78 Loss: 0.455461\n",
            "\tTraining batch 79 Loss: 1.235245\n",
            "\tTraining batch 80 Loss: 0.812773\n",
            "\tTraining batch 81 Loss: 0.099659\n",
            "\tTraining batch 82 Loss: 0.168801\n",
            "\tTraining batch 83 Loss: 0.345199\n",
            "\tTraining batch 84 Loss: 0.018435\n",
            "\tTraining batch 85 Loss: 0.121225\n",
            "\tTraining batch 86 Loss: 0.015280\n",
            "\tTraining batch 87 Loss: 0.156005\n",
            "\tTraining batch 88 Loss: 0.116588\n",
            "\tTraining batch 89 Loss: 0.422638\n",
            "\tTraining batch 90 Loss: 0.051158\n",
            "\tTraining batch 91 Loss: 0.022370\n",
            "\tTraining batch 92 Loss: 0.013583\n",
            "\tTraining batch 93 Loss: 0.108847\n",
            "\tTraining batch 94 Loss: 0.096690\n",
            "\tTraining batch 95 Loss: 0.161009\n",
            "\tTraining batch 96 Loss: 0.593562\n",
            "\tTraining batch 97 Loss: 0.026286\n",
            "\tTraining batch 98 Loss: 0.009697\n",
            "\tTraining batch 99 Loss: 0.042548\n",
            "\tTraining batch 100 Loss: 0.040813\n",
            "\tTraining batch 101 Loss: 0.047860\n",
            "\tTraining batch 102 Loss: 0.025775\n",
            "\tTraining batch 103 Loss: 0.005204\n",
            "\tTraining batch 104 Loss: 1.388983\n",
            "\tTraining batch 105 Loss: 0.472453\n",
            "\tTraining batch 106 Loss: 0.113142\n",
            "\tTraining batch 107 Loss: 0.002495\n",
            "\tTraining batch 108 Loss: 0.001005\n",
            "\tTraining batch 109 Loss: 0.026468\n",
            "\tTraining batch 110 Loss: 0.010026\n",
            "\tTraining batch 111 Loss: 0.007434\n",
            "\tTraining batch 112 Loss: 0.001209\n",
            "\tTraining batch 113 Loss: 0.012641\n",
            "\tTraining batch 114 Loss: 0.056121\n",
            "\tTraining batch 115 Loss: 0.014007\n",
            "\tTraining batch 116 Loss: 0.403841\n",
            "\tTraining batch 117 Loss: 0.004413\n",
            "\tTraining batch 118 Loss: 0.003400\n",
            "\tTraining batch 119 Loss: 0.008762\n",
            "\tTraining batch 120 Loss: 0.001188\n",
            "\tTraining batch 121 Loss: 0.061401\n",
            "\tTraining batch 122 Loss: 0.007325\n",
            "\tTraining batch 123 Loss: 0.010963\n",
            "\tTraining batch 124 Loss: 0.041937\n",
            "\tTraining batch 125 Loss: 0.012476\n",
            "\tTraining batch 126 Loss: 0.044405\n",
            "\tTraining batch 127 Loss: 0.183634\n",
            "\tTraining batch 128 Loss: 0.050500\n",
            "\tTraining batch 129 Loss: 0.000203\n",
            "\tTraining batch 130 Loss: 0.008157\n",
            "\tTraining batch 131 Loss: 0.005645\n",
            "\tTraining batch 132 Loss: 0.004915\n",
            "\tTraining batch 133 Loss: 0.002393\n",
            "\tTraining batch 134 Loss: 0.100875\n",
            "\tTraining batch 135 Loss: 0.001702\n",
            "\tTraining batch 136 Loss: 0.011012\n",
            "\tTraining batch 137 Loss: 0.016526\n",
            "\tTraining batch 138 Loss: 0.004986\n",
            "\tTraining batch 139 Loss: 0.003287\n",
            "\tTraining batch 140 Loss: 0.000850\n",
            "\tTraining batch 141 Loss: 0.002759\n",
            "\tTraining batch 142 Loss: 0.002786\n",
            "\tTraining batch 143 Loss: 0.019096\n",
            "\tTraining batch 144 Loss: 0.001313\n",
            "\tTraining batch 145 Loss: 0.001611\n",
            "\tTraining batch 146 Loss: 0.004333\n",
            "\tTraining batch 147 Loss: 0.006046\n",
            "\tTraining batch 148 Loss: 0.032879\n",
            "\tTraining batch 149 Loss: 0.003506\n",
            "\tTraining batch 150 Loss: 0.023909\n",
            "\tTraining batch 151 Loss: 0.075866\n",
            "\tTraining batch 152 Loss: 0.001526\n",
            "\tTraining batch 153 Loss: 0.000662\n",
            "\tTraining batch 154 Loss: 0.001517\n",
            "\tTraining batch 155 Loss: 0.001614\n",
            "\tTraining batch 156 Loss: 0.057341\n",
            "\tTraining batch 157 Loss: 0.006806\n",
            "\tTraining batch 158 Loss: 0.054753\n",
            "\tTraining batch 159 Loss: 0.004044\n",
            "\tTraining batch 160 Loss: 0.005514\n",
            "\tTraining batch 161 Loss: 0.060122\n",
            "\tTraining batch 162 Loss: 0.001540\n",
            "\tTraining batch 163 Loss: 0.000482\n",
            "\tTraining batch 164 Loss: 0.003330\n",
            "\tTraining batch 165 Loss: 0.001674\n",
            "\tTraining batch 166 Loss: 0.005789\n",
            "\tTraining batch 167 Loss: 0.000548\n",
            "\tTraining batch 168 Loss: 0.000786\n",
            "\tTraining batch 169 Loss: 0.001429\n",
            "\tTraining batch 170 Loss: 0.001828\n",
            "\tTraining batch 171 Loss: 0.000993\n",
            "\tTraining batch 172 Loss: 0.013188\n",
            "\tTraining batch 173 Loss: 0.020209\n",
            "\tTraining batch 174 Loss: 0.000239\n",
            "\tTraining batch 175 Loss: 0.001444\n",
            "\tTraining batch 176 Loss: 0.018009\n",
            "\tTraining batch 177 Loss: 0.026702\n",
            "\tTraining batch 178 Loss: 0.002072\n",
            "\tTraining batch 179 Loss: 0.008073\n",
            "\tTraining batch 180 Loss: 0.016325\n",
            "\tTraining batch 181 Loss: 0.005365\n",
            "\tTraining batch 182 Loss: 0.000546\n",
            "\tTraining batch 183 Loss: 0.008802\n",
            "\tTraining batch 184 Loss: 0.000275\n",
            "\tTraining batch 185 Loss: 0.000554\n",
            "\tTraining batch 186 Loss: 0.001353\n",
            "\tTraining batch 187 Loss: 0.023507\n",
            "\tTraining batch 188 Loss: 0.007440\n",
            "\tTraining batch 189 Loss: 0.002489\n",
            "\tTraining batch 190 Loss: 0.000714\n",
            "\tTraining batch 191 Loss: 0.001020\n",
            "\tTraining batch 192 Loss: 0.000773\n",
            "\tTraining batch 193 Loss: 0.004215\n",
            "\tTraining batch 194 Loss: 0.000094\n",
            "\tTraining batch 195 Loss: 0.024281\n",
            "\tTraining batch 196 Loss: 0.011843\n",
            "\tTraining batch 197 Loss: 0.000061\n",
            "\tTraining batch 198 Loss: 0.000149\n",
            "\tTraining batch 199 Loss: 0.003388\n",
            "\tTraining batch 200 Loss: 0.015151\n",
            "\tTraining batch 201 Loss: 0.026258\n",
            "\tTraining batch 202 Loss: 0.000240\n",
            "\tTraining batch 203 Loss: 0.000411\n",
            "\tTraining batch 204 Loss: 0.001493\n",
            "\tTraining batch 205 Loss: 0.000058\n",
            "\tTraining batch 206 Loss: 0.001205\n",
            "\tTraining batch 207 Loss: 0.000085\n",
            "\tTraining batch 208 Loss: 0.005544\n",
            "\tTraining batch 209 Loss: 0.001739\n",
            "\tTraining batch 210 Loss: 0.034396\n",
            "Training set: Average loss: 0.340665\n",
            "Test set: Average loss: 0.0001, Accuracy: 360/360 (100%)\n",
            "\n",
            "Epoch: 2\n",
            "\tTraining batch 1 Loss: 0.012923\n",
            "\tTraining batch 2 Loss: 0.000543\n",
            "\tTraining batch 3 Loss: 0.000072\n",
            "\tTraining batch 4 Loss: 0.022618\n",
            "\tTraining batch 5 Loss: 0.000493\n",
            "\tTraining batch 6 Loss: 0.000276\n",
            "\tTraining batch 7 Loss: 0.002830\n",
            "\tTraining batch 8 Loss: 0.005738\n",
            "\tTraining batch 9 Loss: 0.047640\n",
            "\tTraining batch 10 Loss: 0.000278\n",
            "\tTraining batch 11 Loss: 0.001163\n",
            "\tTraining batch 12 Loss: 0.037144\n",
            "\tTraining batch 13 Loss: 0.000720\n",
            "\tTraining batch 14 Loss: 0.008569\n",
            "\tTraining batch 15 Loss: 0.006827\n",
            "\tTraining batch 16 Loss: 0.000296\n",
            "\tTraining batch 17 Loss: 0.000051\n",
            "\tTraining batch 18 Loss: 0.000194\n",
            "\tTraining batch 19 Loss: 0.001442\n",
            "\tTraining batch 20 Loss: 0.009670\n",
            "\tTraining batch 21 Loss: 0.005615\n",
            "\tTraining batch 22 Loss: 0.001121\n",
            "\tTraining batch 23 Loss: 0.003634\n",
            "\tTraining batch 24 Loss: 0.004269\n",
            "\tTraining batch 25 Loss: 0.005728\n",
            "\tTraining batch 26 Loss: 0.003872\n",
            "\tTraining batch 27 Loss: 0.000237\n",
            "\tTraining batch 28 Loss: 0.000745\n",
            "\tTraining batch 29 Loss: 0.001580\n",
            "\tTraining batch 30 Loss: 0.006958\n",
            "\tTraining batch 31 Loss: 0.000206\n",
            "\tTraining batch 32 Loss: 0.000039\n",
            "\tTraining batch 33 Loss: 0.000343\n",
            "\tTraining batch 34 Loss: 0.017637\n",
            "\tTraining batch 35 Loss: 0.007906\n",
            "\tTraining batch 36 Loss: 0.007667\n",
            "\tTraining batch 37 Loss: 0.000269\n",
            "\tTraining batch 38 Loss: 0.000753\n",
            "\tTraining batch 39 Loss: 0.000071\n",
            "\tTraining batch 40 Loss: 0.000043\n",
            "\tTraining batch 41 Loss: 0.000041\n",
            "\tTraining batch 42 Loss: 0.000578\n",
            "\tTraining batch 43 Loss: 0.000241\n",
            "\tTraining batch 44 Loss: 0.000047\n",
            "\tTraining batch 45 Loss: 0.000628\n",
            "\tTraining batch 46 Loss: 0.000425\n",
            "\tTraining batch 47 Loss: 0.008607\n",
            "\tTraining batch 48 Loss: 0.004794\n",
            "\tTraining batch 49 Loss: 0.000298\n",
            "\tTraining batch 50 Loss: 0.003584\n",
            "\tTraining batch 51 Loss: 0.000381\n",
            "\tTraining batch 52 Loss: 0.010246\n",
            "\tTraining batch 53 Loss: 0.001487\n",
            "\tTraining batch 54 Loss: 0.003978\n",
            "\tTraining batch 55 Loss: 0.000319\n",
            "\tTraining batch 56 Loss: 0.000176\n",
            "\tTraining batch 57 Loss: 0.000142\n",
            "\tTraining batch 58 Loss: 0.005834\n",
            "\tTraining batch 59 Loss: 0.002903\n",
            "\tTraining batch 60 Loss: 0.083023\n",
            "\tTraining batch 61 Loss: 0.000085\n",
            "\tTraining batch 62 Loss: 6.554142\n",
            "\tTraining batch 63 Loss: 0.007740\n",
            "\tTraining batch 64 Loss: 1.299853\n",
            "\tTraining batch 65 Loss: 0.005301\n",
            "\tTraining batch 66 Loss: 0.339313\n",
            "\tTraining batch 67 Loss: 1.395606\n",
            "\tTraining batch 68 Loss: 0.201910\n",
            "\tTraining batch 69 Loss: 0.671291\n",
            "\tTraining batch 70 Loss: 0.175827\n",
            "\tTraining batch 71 Loss: 0.086365\n",
            "\tTraining batch 72 Loss: 0.311718\n",
            "\tTraining batch 73 Loss: 1.398919\n",
            "\tTraining batch 74 Loss: 1.506932\n",
            "\tTraining batch 75 Loss: 0.037387\n",
            "\tTraining batch 76 Loss: 0.115181\n",
            "\tTraining batch 77 Loss: 0.052074\n",
            "\tTraining batch 78 Loss: 0.585710\n",
            "\tTraining batch 79 Loss: 0.251182\n",
            "\tTraining batch 80 Loss: 2.255746\n",
            "\tTraining batch 81 Loss: 0.574889\n",
            "\tTraining batch 82 Loss: 0.057689\n",
            "\tTraining batch 83 Loss: 0.240587\n",
            "\tTraining batch 84 Loss: 0.183028\n",
            "\tTraining batch 85 Loss: 0.130316\n",
            "\tTraining batch 86 Loss: 0.138481\n",
            "\tTraining batch 87 Loss: 0.031268\n",
            "\tTraining batch 88 Loss: 0.013809\n",
            "\tTraining batch 89 Loss: 0.384588\n",
            "\tTraining batch 90 Loss: 0.117180\n",
            "\tTraining batch 91 Loss: 0.028472\n",
            "\tTraining batch 92 Loss: 0.009472\n",
            "\tTraining batch 93 Loss: 0.170294\n",
            "\tTraining batch 94 Loss: 0.036180\n",
            "\tTraining batch 95 Loss: 0.341343\n",
            "\tTraining batch 96 Loss: 1.422800\n",
            "\tTraining batch 97 Loss: 0.115647\n",
            "\tTraining batch 98 Loss: 0.023514\n",
            "\tTraining batch 99 Loss: 0.163457\n",
            "\tTraining batch 100 Loss: 0.088143\n",
            "\tTraining batch 101 Loss: 0.448075\n",
            "\tTraining batch 102 Loss: 0.024962\n",
            "\tTraining batch 103 Loss: 0.015414\n",
            "\tTraining batch 104 Loss: 1.388465\n",
            "\tTraining batch 105 Loss: 0.959767\n",
            "\tTraining batch 106 Loss: 0.118989\n",
            "\tTraining batch 107 Loss: 0.012376\n",
            "\tTraining batch 108 Loss: 0.040081\n",
            "\tTraining batch 109 Loss: 0.824439\n",
            "\tTraining batch 110 Loss: 0.001323\n",
            "\tTraining batch 111 Loss: 0.200458\n",
            "\tTraining batch 112 Loss: 0.009852\n",
            "\tTraining batch 113 Loss: 0.005896\n",
            "\tTraining batch 114 Loss: 0.355037\n",
            "\tTraining batch 115 Loss: 0.463900\n",
            "\tTraining batch 116 Loss: 0.372247\n",
            "\tTraining batch 117 Loss: 0.080613\n",
            "\tTraining batch 118 Loss: 0.288672\n",
            "\tTraining batch 119 Loss: 0.161658\n",
            "\tTraining batch 120 Loss: 0.004890\n",
            "\tTraining batch 121 Loss: 0.060505\n",
            "\tTraining batch 122 Loss: 0.763051\n",
            "\tTraining batch 123 Loss: 0.517216\n",
            "\tTraining batch 124 Loss: 0.020848\n",
            "\tTraining batch 125 Loss: 0.009380\n",
            "\tTraining batch 126 Loss: 0.027562\n",
            "\tTraining batch 127 Loss: 0.113162\n",
            "\tTraining batch 128 Loss: 0.024618\n",
            "\tTraining batch 129 Loss: 0.084711\n",
            "\tTraining batch 130 Loss: 0.132662\n",
            "\tTraining batch 131 Loss: 0.035368\n",
            "\tTraining batch 132 Loss: 0.026073\n",
            "\tTraining batch 133 Loss: 0.008177\n",
            "\tTraining batch 134 Loss: 0.121284\n",
            "\tTraining batch 135 Loss: 0.004769\n",
            "\tTraining batch 136 Loss: 0.008117\n",
            "\tTraining batch 137 Loss: 0.034974\n",
            "\tTraining batch 138 Loss: 0.003438\n",
            "\tTraining batch 139 Loss: 0.004618\n",
            "\tTraining batch 140 Loss: 0.000113\n",
            "\tTraining batch 141 Loss: 0.000144\n",
            "\tTraining batch 142 Loss: 0.010256\n",
            "\tTraining batch 143 Loss: 0.223940\n",
            "\tTraining batch 144 Loss: 0.000117\n",
            "\tTraining batch 145 Loss: 0.001417\n",
            "\tTraining batch 146 Loss: 0.014939\n",
            "\tTraining batch 147 Loss: 0.019737\n",
            "\tTraining batch 148 Loss: 0.044982\n",
            "\tTraining batch 149 Loss: 0.000685\n",
            "\tTraining batch 150 Loss: 0.061542\n",
            "\tTraining batch 151 Loss: 0.204090\n",
            "\tTraining batch 152 Loss: 0.001348\n",
            "\tTraining batch 153 Loss: 0.000200\n",
            "\tTraining batch 154 Loss: 0.002343\n",
            "\tTraining batch 155 Loss: 0.000466\n",
            "\tTraining batch 156 Loss: 0.045521\n",
            "\tTraining batch 157 Loss: 0.004313\n",
            "\tTraining batch 158 Loss: 0.368970\n",
            "\tTraining batch 159 Loss: 0.006258\n",
            "\tTraining batch 160 Loss: 0.046025\n",
            "\tTraining batch 161 Loss: 0.073532\n",
            "\tTraining batch 162 Loss: 0.001137\n",
            "\tTraining batch 163 Loss: 0.000321\n",
            "\tTraining batch 164 Loss: 0.002747\n",
            "\tTraining batch 165 Loss: 0.000213\n",
            "\tTraining batch 166 Loss: 0.002812\n",
            "\tTraining batch 167 Loss: 0.003046\n",
            "\tTraining batch 168 Loss: 0.025844\n",
            "\tTraining batch 169 Loss: 0.002520\n",
            "\tTraining batch 170 Loss: 0.001168\n",
            "\tTraining batch 171 Loss: 0.003212\n",
            "\tTraining batch 172 Loss: 0.333907\n",
            "\tTraining batch 173 Loss: 0.003234\n",
            "\tTraining batch 174 Loss: 0.000311\n",
            "\tTraining batch 175 Loss: 0.002034\n",
            "\tTraining batch 176 Loss: 0.275786\n",
            "\tTraining batch 177 Loss: 0.701654\n",
            "\tTraining batch 178 Loss: 0.003338\n",
            "\tTraining batch 179 Loss: 0.022652\n",
            "\tTraining batch 180 Loss: 0.002603\n",
            "\tTraining batch 181 Loss: 0.076511\n",
            "\tTraining batch 182 Loss: 0.007921\n",
            "\tTraining batch 183 Loss: 0.002865\n",
            "\tTraining batch 184 Loss: 0.022239\n",
            "\tTraining batch 185 Loss: 0.003719\n",
            "\tTraining batch 186 Loss: 0.006543\n",
            "\tTraining batch 187 Loss: 0.000676\n",
            "\tTraining batch 188 Loss: 0.031473\n",
            "\tTraining batch 189 Loss: 0.691061\n",
            "\tTraining batch 190 Loss: 0.083160\n",
            "\tTraining batch 191 Loss: 0.077881\n",
            "\tTraining batch 192 Loss: 0.000428\n",
            "\tTraining batch 193 Loss: 0.054846\n",
            "\tTraining batch 194 Loss: 0.000264\n",
            "\tTraining batch 195 Loss: 0.143824\n",
            "\tTraining batch 196 Loss: 0.047424\n",
            "\tTraining batch 197 Loss: 0.000089\n",
            "\tTraining batch 198 Loss: 0.000248\n",
            "\tTraining batch 199 Loss: 0.027209\n",
            "\tTraining batch 200 Loss: 0.070048\n",
            "\tTraining batch 201 Loss: 0.582101\n",
            "\tTraining batch 202 Loss: 0.006060\n",
            "\tTraining batch 203 Loss: 0.007548\n",
            "\tTraining batch 204 Loss: 0.004096\n",
            "\tTraining batch 205 Loss: 0.003708\n",
            "\tTraining batch 206 Loss: 0.480973\n",
            "\tTraining batch 207 Loss: 0.000222\n",
            "\tTraining batch 208 Loss: 0.028260\n",
            "\tTraining batch 209 Loss: 0.007596\n",
            "\tTraining batch 210 Loss: 0.351940\n",
            "Training set: Average loss: 0.166654\n",
            "Test set: Average loss: 0.0155, Accuracy: 347/360 (96%)\n",
            "\n",
            "Epoch: 3\n",
            "\tTraining batch 1 Loss: 0.007191\n",
            "\tTraining batch 2 Loss: 0.001769\n",
            "\tTraining batch 3 Loss: 0.000615\n",
            "\tTraining batch 4 Loss: 0.007139\n",
            "\tTraining batch 5 Loss: 0.000268\n",
            "\tTraining batch 6 Loss: 0.000785\n",
            "\tTraining batch 7 Loss: 0.467481\n",
            "\tTraining batch 8 Loss: 0.048532\n",
            "\tTraining batch 9 Loss: 0.026050\n",
            "\tTraining batch 10 Loss: 0.000160\n",
            "\tTraining batch 11 Loss: 0.009490\n",
            "\tTraining batch 12 Loss: 0.607739\n",
            "\tTraining batch 13 Loss: 0.026863\n",
            "\tTraining batch 14 Loss: 0.059886\n",
            "\tTraining batch 15 Loss: 0.021368\n",
            "\tTraining batch 16 Loss: 1.067842\n",
            "\tTraining batch 17 Loss: 0.033067\n",
            "\tTraining batch 18 Loss: 0.102281\n",
            "\tTraining batch 19 Loss: 0.014471\n",
            "\tTraining batch 20 Loss: 0.084807\n",
            "\tTraining batch 21 Loss: 0.077967\n",
            "\tTraining batch 22 Loss: 0.002669\n",
            "\tTraining batch 23 Loss: 0.009423\n",
            "\tTraining batch 24 Loss: 0.041297\n",
            "\tTraining batch 25 Loss: 0.030262\n",
            "\tTraining batch 26 Loss: 0.013144\n",
            "\tTraining batch 27 Loss: 0.002861\n",
            "\tTraining batch 28 Loss: 0.066519\n",
            "\tTraining batch 29 Loss: 0.130672\n",
            "\tTraining batch 30 Loss: 0.028640\n",
            "\tTraining batch 31 Loss: 0.002165\n",
            "\tTraining batch 32 Loss: 0.000300\n",
            "\tTraining batch 33 Loss: 0.000162\n",
            "\tTraining batch 34 Loss: 0.013857\n",
            "\tTraining batch 35 Loss: 0.010534\n",
            "\tTraining batch 36 Loss: 0.011325\n",
            "\tTraining batch 37 Loss: 0.002297\n",
            "\tTraining batch 38 Loss: 0.003730\n",
            "\tTraining batch 39 Loss: 0.000018\n",
            "\tTraining batch 40 Loss: 0.000039\n",
            "\tTraining batch 41 Loss: 0.001119\n",
            "\tTraining batch 42 Loss: 0.000188\n",
            "\tTraining batch 43 Loss: 0.000897\n",
            "\tTraining batch 44 Loss: 0.000242\n",
            "\tTraining batch 45 Loss: 0.005382\n",
            "\tTraining batch 46 Loss: 0.000490\n",
            "\tTraining batch 47 Loss: 0.035531\n",
            "\tTraining batch 48 Loss: 0.016328\n",
            "\tTraining batch 49 Loss: 0.045868\n",
            "\tTraining batch 50 Loss: 0.000744\n",
            "\tTraining batch 51 Loss: 0.000164\n",
            "\tTraining batch 52 Loss: 0.004461\n",
            "\tTraining batch 53 Loss: 0.005539\n",
            "\tTraining batch 54 Loss: 0.006300\n",
            "\tTraining batch 55 Loss: 0.000127\n",
            "\tTraining batch 56 Loss: 0.000204\n",
            "\tTraining batch 57 Loss: 0.000033\n",
            "\tTraining batch 58 Loss: 0.003984\n",
            "\tTraining batch 59 Loss: 0.007786\n",
            "\tTraining batch 60 Loss: 1.297039\n",
            "\tTraining batch 61 Loss: 0.001384\n",
            "\tTraining batch 62 Loss: 3.718288\n",
            "\tTraining batch 63 Loss: 0.299772\n",
            "\tTraining batch 64 Loss: 1.188461\n",
            "\tTraining batch 65 Loss: 0.012063\n",
            "\tTraining batch 66 Loss: 0.178503\n",
            "\tTraining batch 67 Loss: 0.015760\n",
            "\tTraining batch 68 Loss: 0.009547\n",
            "\tTraining batch 69 Loss: 0.082502\n",
            "\tTraining batch 70 Loss: 0.007133\n",
            "\tTraining batch 71 Loss: 0.000772\n",
            "\tTraining batch 72 Loss: 0.007288\n",
            "\tTraining batch 73 Loss: 0.252963\n",
            "\tTraining batch 74 Loss: 0.046370\n",
            "\tTraining batch 75 Loss: 0.007041\n",
            "\tTraining batch 76 Loss: 0.102354\n",
            "\tTraining batch 77 Loss: 0.003002\n",
            "\tTraining batch 78 Loss: 0.000682\n",
            "\tTraining batch 79 Loss: 0.012214\n",
            "\tTraining batch 80 Loss: 0.156820\n",
            "\tTraining batch 81 Loss: 0.021734\n",
            "\tTraining batch 82 Loss: 0.000551\n",
            "\tTraining batch 83 Loss: 0.331326\n",
            "\tTraining batch 84 Loss: 0.000901\n",
            "\tTraining batch 85 Loss: 0.007483\n",
            "\tTraining batch 86 Loss: 0.003881\n",
            "\tTraining batch 87 Loss: 0.003353\n",
            "\tTraining batch 88 Loss: 0.002452\n",
            "\tTraining batch 89 Loss: 0.010407\n",
            "\tTraining batch 90 Loss: 0.008451\n",
            "\tTraining batch 91 Loss: 0.006709\n",
            "\tTraining batch 92 Loss: 0.004213\n",
            "\tTraining batch 93 Loss: 0.013030\n",
            "\tTraining batch 94 Loss: 0.001201\n",
            "\tTraining batch 95 Loss: 0.065385\n",
            "\tTraining batch 96 Loss: 0.779434\n",
            "\tTraining batch 97 Loss: 0.030282\n",
            "\tTraining batch 98 Loss: 0.009355\n",
            "\tTraining batch 99 Loss: 0.000520\n",
            "\tTraining batch 100 Loss: 0.001982\n",
            "\tTraining batch 101 Loss: 0.001856\n",
            "\tTraining batch 102 Loss: 0.003136\n",
            "\tTraining batch 103 Loss: 0.006467\n",
            "\tTraining batch 104 Loss: 0.247126\n",
            "\tTraining batch 105 Loss: 0.000523\n",
            "\tTraining batch 106 Loss: 0.019807\n",
            "\tTraining batch 107 Loss: 0.001010\n",
            "\tTraining batch 108 Loss: 0.000708\n",
            "\tTraining batch 109 Loss: 0.006085\n",
            "\tTraining batch 110 Loss: 0.001418\n",
            "\tTraining batch 111 Loss: 0.005759\n",
            "\tTraining batch 112 Loss: 0.008608\n",
            "\tTraining batch 113 Loss: 0.007059\n",
            "\tTraining batch 114 Loss: 0.034588\n",
            "\tTraining batch 115 Loss: 0.060019\n",
            "\tTraining batch 116 Loss: 0.028781\n",
            "\tTraining batch 117 Loss: 0.002429\n",
            "\tTraining batch 118 Loss: 0.043674\n",
            "\tTraining batch 119 Loss: 0.003208\n",
            "\tTraining batch 120 Loss: 0.000418\n",
            "\tTraining batch 121 Loss: 0.002325\n",
            "\tTraining batch 122 Loss: 0.001143\n",
            "\tTraining batch 123 Loss: 0.005713\n",
            "\tTraining batch 124 Loss: 0.016842\n",
            "\tTraining batch 125 Loss: 0.001352\n",
            "\tTraining batch 126 Loss: 0.002925\n",
            "\tTraining batch 127 Loss: 0.019753\n",
            "\tTraining batch 128 Loss: 0.006720\n",
            "\tTraining batch 129 Loss: 0.002456\n",
            "\tTraining batch 130 Loss: 0.008571\n",
            "\tTraining batch 131 Loss: 0.002392\n",
            "\tTraining batch 132 Loss: 0.001088\n",
            "\tTraining batch 133 Loss: 0.000796\n",
            "\tTraining batch 134 Loss: 0.025453\n",
            "\tTraining batch 135 Loss: 0.000461\n",
            "\tTraining batch 136 Loss: 0.000013\n",
            "\tTraining batch 137 Loss: 0.012367\n",
            "\tTraining batch 138 Loss: 0.002209\n",
            "\tTraining batch 139 Loss: 0.001782\n",
            "\tTraining batch 140 Loss: 0.000259\n",
            "\tTraining batch 141 Loss: 0.000083\n",
            "\tTraining batch 142 Loss: 0.000462\n",
            "\tTraining batch 143 Loss: 0.016774\n",
            "\tTraining batch 144 Loss: 0.000021\n",
            "\tTraining batch 145 Loss: 0.000713\n",
            "\tTraining batch 146 Loss: 0.002044\n",
            "\tTraining batch 147 Loss: 0.004696\n",
            "\tTraining batch 148 Loss: 0.006073\n",
            "\tTraining batch 149 Loss: 0.000593\n",
            "\tTraining batch 150 Loss: 0.017131\n",
            "\tTraining batch 151 Loss: 0.008938\n",
            "\tTraining batch 152 Loss: 0.010077\n",
            "\tTraining batch 153 Loss: 0.000916\n",
            "\tTraining batch 154 Loss: 0.001232\n",
            "\tTraining batch 155 Loss: 0.000514\n",
            "\tTraining batch 156 Loss: 0.000129\n",
            "\tTraining batch 157 Loss: 0.001834\n",
            "\tTraining batch 158 Loss: 0.000257\n",
            "\tTraining batch 159 Loss: 0.003592\n",
            "\tTraining batch 160 Loss: 0.002650\n",
            "\tTraining batch 161 Loss: 0.000414\n",
            "\tTraining batch 162 Loss: 0.001140\n",
            "\tTraining batch 163 Loss: 0.000027\n",
            "\tTraining batch 164 Loss: 0.002478\n",
            "\tTraining batch 165 Loss: 0.000081\n",
            "\tTraining batch 166 Loss: 0.000983\n",
            "\tTraining batch 167 Loss: 0.000789\n",
            "\tTraining batch 168 Loss: 0.001574\n",
            "\tTraining batch 169 Loss: 0.001387\n",
            "\tTraining batch 170 Loss: 0.003324\n",
            "\tTraining batch 171 Loss: 0.002125\n",
            "\tTraining batch 172 Loss: 0.011793\n",
            "\tTraining batch 173 Loss: 0.003430\n",
            "\tTraining batch 174 Loss: 0.000040\n",
            "\tTraining batch 175 Loss: 0.002053\n",
            "\tTraining batch 176 Loss: 0.002679\n",
            "\tTraining batch 177 Loss: 0.006072\n",
            "\tTraining batch 178 Loss: 0.000860\n",
            "\tTraining batch 179 Loss: 0.017146\n",
            "\tTraining batch 180 Loss: 0.015738\n",
            "\tTraining batch 181 Loss: 0.002067\n",
            "\tTraining batch 182 Loss: 0.000465\n",
            "\tTraining batch 183 Loss: 0.003017\n",
            "\tTraining batch 184 Loss: 0.000243\n",
            "\tTraining batch 185 Loss: 0.000013\n",
            "\tTraining batch 186 Loss: 0.001388\n",
            "\tTraining batch 187 Loss: 0.003996\n",
            "\tTraining batch 188 Loss: 0.001551\n",
            "\tTraining batch 189 Loss: 0.000181\n",
            "\tTraining batch 190 Loss: 0.000251\n",
            "\tTraining batch 191 Loss: 0.000073\n",
            "\tTraining batch 192 Loss: 0.000166\n",
            "\tTraining batch 193 Loss: 0.000056\n",
            "\tTraining batch 194 Loss: 0.000364\n",
            "\tTraining batch 195 Loss: 0.006806\n",
            "\tTraining batch 196 Loss: 0.002134\n",
            "\tTraining batch 197 Loss: 0.000200\n",
            "\tTraining batch 198 Loss: 0.000020\n",
            "\tTraining batch 199 Loss: 0.001300\n",
            "\tTraining batch 200 Loss: 0.028524\n",
            "\tTraining batch 201 Loss: 0.016049\n",
            "\tTraining batch 202 Loss: 0.000062\n",
            "\tTraining batch 203 Loss: 0.000107\n",
            "\tTraining batch 204 Loss: 0.000083\n",
            "\tTraining batch 205 Loss: 0.000075\n",
            "\tTraining batch 206 Loss: 0.000132\n",
            "\tTraining batch 207 Loss: 0.000063\n",
            "\tTraining batch 208 Loss: 0.001929\n",
            "\tTraining batch 209 Loss: 0.003279\n",
            "\tTraining batch 210 Loss: 0.022403\n",
            "Training set: Average loss: 0.060575\n",
            "Test set: Average loss: 0.0001, Accuracy: 360/360 (100%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkOWGwcjzTPL",
        "colab_type": "code",
        "outputId": "ac1beb11-6b37-4695-d8e1-d64b2f685881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(epoch_nums, training_loss)\n",
        "plt.plot(epoch_nums, validation_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIBk6yrT6mkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the model\n",
        "model = model.to('cpu')\n",
        "torch.save(model, 'test.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}